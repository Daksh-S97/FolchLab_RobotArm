{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.dobot_api import DobotApiDashboard, DobotApiMove # deps is a folder and we are importing 2 classes from a file within the folder, these classes \n",
    "# allow us to control the robot informationally and movement oriented \n",
    "from deps import utils # importing the module utils from a file within the folder deps, don't import all files from deps, just utils\n",
    "from deps import cv_core # this module houses all of the functions that are controlling and dealing with the camera and what it's doing\n",
    "import numpy as np # numpy is a module, we want lots of functions from numpy so we import full module\n",
    "import matplotlib.pyplot as plt # module that allows plotting, not used \n",
    "import cv2 # open cv, computer vision module with own GUI\n",
    "from pynput import keyboard # module allows you to get callbacks from keyboard press (control robot using arrow keys)\n",
    "import time # this module allows you to control timings \n",
    "import ipyparallel as ipp # module that allows code to be launched in parallel with other code\n",
    "import random\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = ipp.Cluster(n=1).start_and_connect_sync() # use this to move the robot with arrow keys and view camera feed at the same time \n",
    "e0 = rc[0]\n",
    "e0.block = False\n",
    "e0.activate('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash = DobotApiDashboard('192.168.1.6', 29999) # dash is the object that is connected to the robot and gets information from the dashboard\n",
    "move = DobotApiMove('192.168.1.6', 30003) # the object that allows you to control the movement of the robot\n",
    "# both classes we input ip address and port that we are communicating with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,{},EnableRobot();'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash.ClearError() # dash is an object and ClearError is a method in dash that we are calling, clears error and warning when robot freezes and turns red\n",
    "dash.EnableRobot() # enables the robot for use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash.DisableRobot() # disables the robot, white letters below is the robot's response as a String, 0 means that everything is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n",
      "Special key pressed: Key.esc\n"
     ]
    }
   ],
   "source": [
    "keys = utils.Keyboard(dash) # initializing the class Keyboard from the module (file that houses functions (methods) and classes utils and we are passing the parameter dash\n",
    "# we are giving the class Keyboard a connection to the robot which is called dash, dash is an object of the class dashboard\n",
    "keys.execute() # Keyboard has a method called execute, use this to record the position of the robot by pressing s. if finished press esc\n",
    "# we want to find corners of well plate so we record the position of the 4 corners and use this information later, just one use of execute of class keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 222.157486, -136.339225,  -69.258835, -151.472656]),\n",
       " array([ 227.648028,  -77.282341,  -69.223312, -138.689148]),\n",
       " array([ 330.491249,  -79.171502,  -71.068153, -133.403992]),\n",
       " array([ 324.89161 , -138.866787,  -70.731972, -143.080811])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.coords # list of arrays with recorded coordinates at positions where s was pressed, coords is an attribute of keys (a variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [np.array([280,  40, -90,  0]), # list of positions from coords of 4 corners around petri dish, called anchors for the laser\n",
    "           np.array([230,  37, -90,  0]),\n",
    "           np.array([220, -14, -90,  0]),\n",
    "           np.array([270, -15, -90,  0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell calculates the grid for the 96 well plate\n",
    "well_plate = utils.assign_corners(keys.coords, reverse=True) # assign_corners is a method in the module utils \n",
    "left_side_points = np.linspace(well_plate['ul'], well_plate['ll'], 12)[:,:2]\n",
    "right_side_points = np.linspace(well_plate['ur'], well_plate['lr'], 12)[:,:2]\n",
    "grid = []\n",
    "for i in range(len(left_side_points)):\n",
    "    x1, y1 = left_side_points[i]\n",
    "    x2, y2 = right_side_points[i]\n",
    "    a = (x2-x1)/(y2-y1)\n",
    "    b = x1 - a*y1\n",
    "    ys = np.linspace(y1,y2,8) \n",
    "    xs = a*ys + b\n",
    "    grid += (list(zip(xs,ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('well_plate_96.npy',np.array(grid)) # saves well plate grid into a file named 'well_plate_96.npy', we want to save this so we can use later and not repeat process\n",
    "#np.load('well_plate_96.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dobot/github/FolchLab_RobotArm/main_v2.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dobot/github/FolchLab_RobotArm/main_v2.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x,y \u001b[39m=\u001b[39m coord\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dobot/github/FolchLab_RobotArm/main_v2.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m move\u001b[39m.\u001b[39mMovL(x,y,\u001b[39m-\u001b[39m\u001b[39m60\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dobot/github/FolchLab_RobotArm/main_v2.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m move\u001b[39m.\u001b[39;49mSync()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dobot/github/FolchLab_RobotArm/main_v2.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m move\u001b[39m.\u001b[39mMovL(x,y,\u001b[39m-\u001b[39m\u001b[39m80\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dobot/github/FolchLab_RobotArm/main_v2.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m move\u001b[39m.\u001b[39mSync()\n",
      "File \u001b[0;32m~/github/FolchLab_RobotArm/deps/dobot_api.py:674\u001b[0m, in \u001b[0;36mDobotApiMove.Sync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSync()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    673\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend_data(string)\n\u001b[0;32m--> 674\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_reply()\n",
      "File \u001b[0;32m~/github/FolchLab_RobotArm/deps/dobot_api.py:159\u001b[0m, in \u001b[0;36mDobotApi.wait_reply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait_reply\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Read the return value\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_dobot\u001b[39m.\u001b[39;49mrecv(\u001b[39m1024\u001b[39;49m)\n\u001b[1;32m    160\u001b[0m     data_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(data, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m     \u001b[39m#self.log(f'Receive from 192.168.1.6:{self.port}: {data_str}')\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#grid = np.load('well_plate_96.npy')\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "for coord in grid:\n",
    "    x,y = coord\n",
    "    move.MovL(x,y,-60,0)\n",
    "    move.Sync()\n",
    "    move.MovL(x,y,-80,0)\n",
    "    move.Sync()\n",
    "    move.MovL(x,y,-60,0)\n",
    "    move.Sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n"
     ]
    }
   ],
   "source": [
    "manmove = utils.ManualMove(move, dash) # used to control robot with keyboard, uses key presses to control robot \n",
    "manmove.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 264.819996,   47.623912,  -79.627495, -122.542259]),\n",
       " array([ 305.828914,  -11.286612,  -79.614861, -122.542191]),\n",
       " array([ 314.047399,   48.680616,  -79.606636, -122.542244]),\n",
       " array([ 257.099979,  -11.794222,  -79.587685, -122.542305])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manmove.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_offset = (manmove.coords[0] - np.array([250, 0, -90, 0]))[:2]\n",
    "np.save('base_offset.npy', base_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_rotation(x, y, angle):\n",
    "    xPrime = x*np.cos(angle) - y*np.sin(angle)\n",
    "    yPrime = x*np.sin(angle) + y*np.cos(angle)\n",
    "\n",
    "    return xPrime, yPrime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "move.MovL(250, 0, -90, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_pose(dash, angle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px0\n",
    "from deps import utils\n",
    "from deps.dobot_api import DobotApiDashboard, DobotApiMove\n",
    "dash = DobotApiDashboard('192.168.1.6', 29999)\n",
    "move = DobotApiMove('192.168.1.6', 30003)\n",
    "manmove = utils.ManualMove(move, dash)\n",
    "manmove.execute()\n",
    "coords = manmove.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = e0.pull('coords')\n",
    "coords = ar.get()\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200'])\n",
    "cv_core.video_test(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('anchors.npy', manmove.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = np.load('anchors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Petri Dish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3279.628] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@3284.069] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@3287.308] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@3289.684] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    }
   ],
   "source": [
    "#anchors = keys.coords\n",
    "anchors = np.load('anchors.npy')\n",
    "# anchors = [np.array([307.315193, -13.865066, -81,  -3.484534]),\n",
    "#  np.array([316.442224,  49.591866, -81,  -3.484533]),\n",
    "#  np.array([268.040607,  48.172406, -81,  -3.484533]),\n",
    "#  np.array([260.923249, -14.987419, -81,  -3.484531])]\n",
    "\n",
    "# anchor positions, positions of the laser that the camera recognizes to create a transformation matrix\n",
    "# allows you to transform pixel coordinates of an object to actual robot coordinates \n",
    "\n",
    "# computer vision stuff\n",
    "\n",
    "cameraMatrix = np.load('./cam_matrices/cam_mtx.npy')\n",
    "dist = np.load('./cam_matrices/dist.npy')\n",
    "newCameraMatrix = np.load('./cam_matrices/newcam_mtx.npy')\n",
    "template = cv2.imread('template.png', 0)\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# cap = cv_core.set_res(cap, cv_corfind_contourse.camera_res_dict['1200'])\n",
    "\n",
    "# cv2.namedWindow('frame',  cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow('frame', 1348, 1011)\n",
    "\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "\n",
    "\n",
    "recorded = []\n",
    "for idx, anchor in enumerate(anchors):\n",
    "    x,y,z,r = anchor\n",
    "    move.MovL(x,y,z,r)\n",
    "    move.Sync()\n",
    "\n",
    "    #while(True):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200'])\n",
    "\n",
    "    cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # creating a GUI window cv2 is a module called open cv which has all the methods related to computer vision\n",
    "    cv2.resizeWindow('frame', 1348, 1011)\n",
    "\n",
    "    ret, frame = cap.read() # how to access camera information, gives single frame that has been captured by the camera at the time of execution\n",
    "    # gives ret which is a boolean (true/false) true if frame captured, frame gives a numpy array that is basically the image (if color, 3 channel array rgb)\n",
    "    \n",
    "    \n",
    "    #frame = cv2.undistort(frame, cameraMatrix, dist, None, newCameraMatrix) # need these 3 parameters to undistort the frame and give a new undistorted frame\n",
    "    \n",
    "    \n",
    "    \n",
    "    plot_img = frame.copy() # create a copy of the variable frame (create a copy of the image)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # turn frame to grayscale \n",
    "    (minVal, maxVal, minLoc, maxLoc)=cv2.minMaxLoc(gray) # gives location of minimum and maximum pixel value, gives coordinates in pixels\n",
    "    a, b = maxLoc # unpack max location into 2 variables, the x and y (a and b) in pixels\n",
    "    \n",
    "    top_left = (a-w, b-h) # make white rectangle around desired maximum pixel values\n",
    "    bottom_right = (a+w, b+h)\n",
    "\n",
    "    mask = np.zeros_like(frame) # brightest part not always the center point, we want to isolate the bright spot, we apply a mask onto the image \n",
    "    # we take a grayscale image and we apply a mask around the bright spot, we create absolute black image \n",
    "    cv2.rectangle(mask,top_left, bottom_right, (255,255,255), -1) # we create filled white rectangle around the bright spot \n",
    "    cv2.rectangle(plot_img,top_left, bottom_right, (255,255,255), 2) # puts white hollow rectangle onto the visual image \n",
    "    result = cv2.bitwise_and(frame.astype('uint8'), mask.astype('uint8')) # we multiply the frame by the mask which leaves only the bright spot\n",
    "    gray_result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY) # result given in rgb and we want to go to grayscale, gives one grayscale array instead of 3 BGR\n",
    "    ret, thresh = cv2.threshold(gray_result,240,255,cv2.THRESH_BINARY) # apply thresholding to take brightest desired pixels and ignore all other values\n",
    "    M = cv2.moments(thresh) # this is what is finding the center by calculating the moments of the bright blob, calculates center of mass of a pixel blob\n",
    "    if M[\"m00\"] != 0:\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        cv2.circle(plot_img, (cX, cY), 5, (0, 255, 0), 2) # drawing a circle around the brightest blob for visual purposes \n",
    "        recorded.append((cX, cY)) # we record the x and y pixel values of the center of the blob so we can map the location onto the robots coordinates, at that point\n",
    "\n",
    "    cv2.imshow('frame',plot_img) # shows the image\n",
    "    #cv2.imwrite(f'anchor_{idx}.jpg', plot_img)\n",
    "    cv2.waitKey(0) # index zero wait key, tells the computer to wait for any key press before continuing execution\n",
    "    cap.release() # releases the camera from control of the computer, disconnects the camera from the computer and empties memory\n",
    "   \n",
    "cv2.destroyAllWindows() # when 4 loop finishes it destroys (closes) the graphical window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_pose(dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Pipette down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "move.MovL(315.233371,  49.014043, -97.242516,  -1.563194)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cell that calculates the transformation matrix \n",
    "xys = [(arr[0], arr[1]) for arr in anchors]\n",
    "robot_coor = utils.assign_corners(xys, reverse=True) # assign corners to the robot coordinates at the 4 corner positions \n",
    "pix_coor = utils.assign_corners(recorded) # assign corners to the pixel coordinates at the 4 corner positions \n",
    "\n",
    "features_mm_to_pixels_dict = {} # setting up an empty dictionary to store the mapping of the corners from coordinate to pixel\n",
    "for key, value in robot_coor.items():\n",
    "    features_mm_to_pixels_dict[value] = pix_coor[key]\n",
    "\n",
    "\n",
    "tf_mtx = cv_core.compute_tf_mtx(features_mm_to_pixels_dict) # method of cv_core module that calculates transformation matrix\n",
    "# takes the dictionary and solves the system of linear equations that gives the transformation matrix and gives the actual relation between the pixels and millimeters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash.ResetRobot()\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "move.JointMovJ(-30,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuboid transfer with Snapshot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@11.899] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "libGL error: MESA-LOADER: failed to open radeonsi: /usr/lib/dri/radeonsi_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: radeonsi\n",
      "libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: swrast\n"
     ]
    }
   ],
   "source": [
    "grid = np.load('well_plate_96.npy')\n",
    "dash.ResetRobot()\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "move.JointMovJ(-30,0,0,0)\n",
    "\n",
    "cont = cv_core.Contours() # define class of methods for cuboid detection, initialize class\n",
    "offset = 40 # create smaller inner circle in petri dish to locate cuboids \n",
    "cap = cv2.VideoCapture(0) # gets access to the camera \n",
    "cap = cv_core.set_res(cap, cv_core.camera_res_dict['1944']) # sets the resolution of the camera to 1200 x 1600\n",
    "\n",
    "cameraMatrix = np.load('./cam_matrices/cam_mtx.npy') # uploading the camera matrices for the calibration of the camera for undistortion\n",
    "dist = np.load('./cam_matrices/dist.npy') \n",
    "newCameraMatrix = np.load('./cam_matrices/newcam_mtx.npy') \n",
    "\n",
    "cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # create window\n",
    "cv2.resizeWindow('frame', 1348, 1011) # set resolution of window\n",
    "idx = 0\n",
    "\n",
    "while(True): # we want a video stream so we want a while loop to continuously take new images until loop is broken\n",
    "    \n",
    "    for i in range(5):   # TODO: Test with this and see if you still need five presses on the spacebar # Sarmad Hassan\n",
    "        ret, frame = cap.read()\n",
    "    #ret, frame = cap.read() \n",
    "    \n",
    "    #frame = cv2.undistort(frame, cameraMatrix, dist, None, newCameraMatrix)\n",
    "\n",
    "    pt = cont.find_contours(frame, 10, offset) # takes the frame and looks for the circle of the petri dish, also looks for the cuboids \n",
    "        \n",
    "    a,b,r = pt # gives the x y coordinates and the radius of the circle\n",
    "    plot_img = frame.copy() # create a copy of the frame so things can be drawn on it without altering original image\n",
    "\n",
    "    cv2.circle(plot_img, (a, b), r, (3, 162, 255), 2) # circles being drawn on image to locate petri dish\n",
    "    cv2.circle(plot_img, (a, b), 1, (0, 0, 255), 3)\n",
    "    cv2.circle(plot_img, (a, b), r - offset, (0, 255, 0), 2) \n",
    "\n",
    "    cv2.drawContours(plot_img, cont.singular, -1,(0,255, 0),2) # two lists of contours (cuboid contours) that we draw on the plotting image\n",
    "    cv2.drawContours(plot_img, cont.clusters, -1,(0,0,255),2) # we do this to distinguish between clusters and cuboids\n",
    "\n",
    "    cv2.putText(plot_img, f\"Found: {len(cont.singular)}\", (25,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2) # how many cuboids it sees\n",
    "\n",
    "    \n",
    "    cv2.imshow('frame',plot_img) # this just displays the image \n",
    "\n",
    "    k = cv2.waitKey(0)\n",
    "    if k == ord('q'): # if you press q, the while loop will break and the video and locating will stop, then the camera is released and windows destroyed\n",
    "        break\n",
    "    elif k == ord('e'):\n",
    "        print(len(cont.singular))\n",
    "        if len(cont.singular) >= 1:\n",
    "            # center = cont.contour_centers(cont.singular)[0]\n",
    "            center = random.choice(cont.contour_centers(cont.singular))\n",
    "            X, Y, _ = tf_mtx @ (center[0], center[1], 1)\n",
    "            move.MovL(X, Y, -60, 0)\n",
    "            move.Sync()\n",
    "            utils.correct_J4_angle(0, dash, move)\n",
    "            # utils.correct_J4_angle(-360, dash, move)\n",
    "            # utils.correct_J4_angle(0, dash, move)\n",
    "            move.RelMovL(0,0,-24) #(0,0,-36)\n",
    "            move.Sync()\n",
    "            utils.correct_J4_angle(120, dash, move)\n",
    "            move.RelMovL(0,0,24) #(0,0,36) #24 works!!\n",
    "            move.Sync()\n",
    "            x,y = grid[idx]\n",
    "            idx += 1\n",
    "            move.MovL(x,y,-60, 120)\n",
    "            move.Sync()\n",
    "            utils.correct_J4_angle(120, dash, move)\n",
    "            move.RelMovL(0,0,-15) #(0,0,-27)\n",
    "            move.Sync()\n",
    "            utils.correct_J4_angle(-100, dash, move)\n",
    "            move.RelMovL(0,0,15) #(0,0,-27)\n",
    "            move.Sync()\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@7.840] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "libGL error: MESA-LOADER: failed to open radeonsi: /usr/lib/dri/radeonsi_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: radeonsi\n",
      "libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: swrast\n"
     ]
    }
   ],
   "source": [
    "cont = cv_core.Contours() # define class of methods for cuboid detection, initialize class\n",
    "offset = 40 # create smaller inner circle in petri dish to locate cuboids \n",
    "cap = cv2.VideoCapture(0) # gets access to the camera \n",
    "cap = cv_core.set_res(cap, cv_core.camera_res_dict['1944'])\n",
    "cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # create window\n",
    "cv2.resizeWindow('frame', 1348, 1011)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    plot_img = frame.copy()\n",
    "    mask = np.zeros_like(frame)\n",
    "\n",
    "    a, b, _ = frame.shape\n",
    "    mask = cv2.circle(mask, (round(b/2), round(a/2)), 700, (255, 255, 255), -1)\n",
    "    masked = cv2.bitwise_and(frame.astype('uint8'), mask.astype('uint8'))\n",
    "\n",
    "    plot_mask = cv2.circle(mask, (round(b/2), round(a/2)), 800, (255, 255, 255), -1)\n",
    "    plot_img = cv2.bitwise_and(plot_img.astype('uint8'), plot_mask.astype('uint8'))\n",
    "    cv2.circle(plot_img, (round(b/2), round(a/2)), 700, (0, 0, 255), 3)\n",
    "    gray = cv2.cvtColor(masked, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY_INV,29,5)\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # dilation = cv2.dilate(thresh,kernel,iterations = 1)\n",
    "    # erosion = cv2.erode(thresh,kernel,iterations = 1)\n",
    "    res = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    contours, hierarchy = cv2.findContours(\n",
    "            res, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = cont.filter_contours(contours, 60, 500)\n",
    "\n",
    "    \n",
    "    cv2.drawContours(plot_img, contours, -1,(0,255, 0),2)\n",
    "    \n",
    "    for c in contours:\n",
    "        M = cv2.moments(c)\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"]) # the function moments gives a mysterious dictionary with elements of matrix, it seems that you gotta divide one element by another to get x&y\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        cv2.putText(plot_img, f\"{cv2.contourArea(c)}\", (cX - 20, cY - 20), # putting the area of the cuboids onto the screen as text next to the cuboid\n",
    "              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 1)\n",
    "\n",
    "    cv2.imshow('frame',plot_img)\n",
    "    k = cv2.waitKey(0)\n",
    "    if k == ord('q'): # if you press q, the while loop will break and the video and locating will stop, then the camera is released and windows destroyed\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b, _ = frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 2592)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = cap.read()  \n",
    "cap.release()  \n",
    "cv2.imshow('frame',frame) # this just displays the image \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # create window\n",
    "cv2.resizeWindow('frame', 1348, 1011) # set resolution of window\n",
    "\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #ret,res = cv2.threshold(gray,125,255,cv2.THRESH_BINARY_INV)\n",
    "ret,res = cv2.threshold(gray,125,255,cv2.THRESH_BINARY_INV)\n",
    "th3 = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY_INV,11,9)\n",
    "\n",
    "cv2.imshow('frame',th3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuboid Recognition and Positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3315.349] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "474\n",
      "474\n",
      "469\n",
      "469\n",
      "469\n",
      "469\n",
      "469\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n"
     ]
    }
   ],
   "source": [
    "cont = cv_core.Contours() # define class of methods for cuboid detection, initialize class\n",
    "\n",
    "def on_change(val): pass\n",
    "\n",
    "offset = 40 # create smaller inner circle in petri dish to locate cuboids \n",
    "cap = cv2.VideoCapture(0) # gets access to the camera \n",
    "cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200']) # sets the resolution of the camera to 1200 x 1600\n",
    "\n",
    "cameraMatrix = np.load('./cam_matrices/cam_mtx.npy') # uploading the camera matrices for the calibration of the camera for undistortion\n",
    "dist = np.load('./cam_matrices/dist.npy') \n",
    "newCameraMatrix = np.load('./cam_matrices/newcam_mtx.npy') \n",
    "\n",
    "cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # create window\n",
    "cv2.resizeWindow('frame', 1348, 1011) # set resolution of window\n",
    "cv2.createTrackbar('Manual Lock', 'frame', 0, 1, on_change) # create trackbar between values of 0 and 1, basically a switch (manual lock of the circle recognition)\n",
    "# stops trying to recognize the petri dish (locked) increases performance of more fps because it recognizes a lot of circles which takes time so this locks it \n",
    "# circle recognition needs optimizing \n",
    "cv2.createTrackbar('Mask Offset', 'frame', offset, 150, on_change) # second trackbar allows control of offset which changes the size of the petri dish detection circle\n",
    "cv2.setMouseCallback('frame', cont.mousecallback) # set a callback function for double clicks of the mouse, this allows us to select cuboids by double clicking\n",
    "# initializes a double click response to select a certain contour on the screen \n",
    "\n",
    "idx = 0 \n",
    "prev_point = (0,0,0)\n",
    "while(True): # we want a video stream so we want a while loop to continuously take new images until loop is broken\n",
    "    ret, frame = cap.read()\n",
    "    #frame = cv2.undistort(frame, cameraMatrix, dist, None, newCameraMatrix)\n",
    "    val = cv2.getTrackbarPos('Manual Lock', 'frame') # checks position of trackbar for manual lock, if trackbar 1, then val = 1, this turns off circle detection\n",
    "    offset = cv2.getTrackbarPos('Mask Offset', 'frame') # the trackbar with the offset, changes the offset value for the circle\n",
    "    if val == 1:\n",
    "        cont.locked = True\n",
    "    else:\n",
    "        cont.locked = False\n",
    "\n",
    "    # if cont.best_circ is None:\n",
    "    #     while cont.best_circ is None:\n",
    "    #         pt = cont.find_contours(frame, 10, offset)\n",
    "    # else:\n",
    "    pt = cont.find_contours(frame, 10, offset) # 10 og,takes the frame and looks for the circle of the petri dish, also looks for the cuboids \n",
    "        \n",
    "    a,b,r = pt # gives the x y coordinates and the radius of the circle\n",
    "    plot_img = frame.copy() # create a copy of the frame so things can be drawn on it without altering original image\n",
    "    cv2.circle(plot_img, (a, b), r, (3, 162, 255), 2) # circles being drawn on image to locate petri dish\n",
    "    cv2.circle(plot_img, (a, b), 1, (0, 0, 255), 3)\n",
    "    cv2.circle(plot_img, (a, b), r - offset, (0, 255, 0), 2)\n",
    "    print(cont.big_circ[2])\n",
    "    cv2.circle(plot_img, cont.big_circ[:2], int(cont.big_circ[2]), (0, 0, 255), 2)\n",
    "    cv2.putText(plot_img, f\"{cont.big_circ[2]*2}px = 60mm\", (cont.big_circ[0]-25, cont.big_circ[1] - cont.big_circ[2] - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2) # how to decide size of cuboids, use diameter of petri dish being 60 mm to approximate size of cuboids\n",
    "        # just a text sign around the biggest circle of the image (petri dish) saying that this circle is 60 mm\n",
    "    \n",
    "    cv2.drawContours(plot_img, cont.singular, -1,(0,255, 0),2) # two lists of contours (cuboid contours) that we draw on the plotting image\n",
    "    cv2.drawContours(plot_img, cont.clusters, -1,(0,0,255),2) # we do this to distinguish between clusters and cuboids \n",
    "\n",
    "    if cont.selected: # if we selected cuboids with the double click, we draw them in blue\n",
    "        cv2.drawContours(plot_img, cont.selected, -1,(255,0,0),2) # BGR, this is blue contour \n",
    "\n",
    "    for c in cont.singular: # we find the centers of the contours similar to how we found centers with the calibration step\n",
    "        # compute the center of the contour\n",
    "        M = cv2.moments(c)\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"]) # the function moments gives a mysterious dictionary with elements of matrix, it seems that you gotta divide one element by another to get x&y\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        \n",
    "        cv2.circle(plot_img, (cX, cY), 2, (0, 0, 255), -1) # just draw a cirlce around the contour \n",
    "        cv2.putText(plot_img, f\"{cX},{cY}\", (cX - 20, cY - 20),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(plot_img, f\"{cv2.contourArea(c)}\", (cX - 20, cY - 20), # putting the area of the cuboids onto the screen as text next to the cuboid\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 1)\n",
    "    #out.write(with_contours)\n",
    "\n",
    "    for c in cont.clusters: # same thing for clustes as above but no printed text for cuboid area or coordinate\n",
    "        # compute the center of the contour\n",
    "        M = cv2.moments(c)\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        # draw the contour and center of the shape on the image\n",
    "        #cv2.drawContours(plot_img, [c], -1, (0, 255, 0), 2)\n",
    "        cv2.circle(plot_img, (cX, cY), 2, (0, 0, 255), -1)\n",
    "        # cv2.putText(plot_img, f\"{cX},{cY}\", (cX - 20, cY - 20),\n",
    "        # cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 255, 0), 1)\n",
    "\n",
    "        # cv2.putText(plot_img, f\"{cv2.contourArea(c)}\", (cX - 20, cY - 20),\n",
    "        # cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "    if prev_point is pt:\n",
    "        idx += 1\n",
    "    else:\n",
    "        idx = 0\n",
    "    prev_point = pt\n",
    "\n",
    "\n",
    "    if idx >= 30: # if circle stays same for awhile then it will show as locked (30 frames the same)\n",
    "        message = 'LOCKED'\n",
    "        color = (0,255,0)\n",
    "    else:\n",
    "        message = 'SEARCHING' # more text messages \n",
    "        color = (0,0,255)\n",
    "\n",
    "    cv2.putText(plot_img, \"TARGET:\", (25,25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2) # text messages, target displays constantly\n",
    "    cv2.putText(plot_img, f\"{message}\", (125,25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2) # displays message locked or searching\n",
    "    cv2.putText(plot_img, f\"Found: {len(cont.singular)}\", (25,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2) # how many cuboids it sees\n",
    "\n",
    "    \n",
    "    cv2.imshow('frame',plot_img) # this just displays the image \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # if you press q, the while loop will break and the video and locating will stop, then the camera is released and windows destroyed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCaptrure(0) # gets access to the camera \n",
    "cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200'])\n",
    "cv_core.video_test(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,{},Sync();'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "M = cv2.moments(cont.selected[0])\n",
    "cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "X, Y, _ = tf_mtx @ (cX, cY, 1)\n",
    "move.MovL(X, Y, -80, 0)\n",
    "move.Sync()\n",
    "#utils.correct_J4_angle(0, dash, move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manmove = utils.ManualMove(move, dash)\n",
    "manmove.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash.DisableRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_error(dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont.selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating min distance between cuboids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mindis(centers):\n",
    "    dis = collections.defaultdict(lambda: np.inf)\n",
    "    for i in range(len(centers)):\n",
    "        x,y = centers[i][0]\n",
    "        #print(x,y)\n",
    "        for j in range(i+1,len(centers)):\n",
    "            x1,y1 = centers[j][0]\n",
    "            distance = math.sqrt(abs(x1-x) ** 2 + abs(y1-y) ** 2)\n",
    "            dis[i] = min(dis[i], distance)\n",
    "            dis[j] = min(dis[j],distance)\n",
    "    return list(sorted(dis.items(), key = lambda item : item[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "centers =np.array([[[663, 552]],\n",
    " \n",
    "        [[661, 554]],\n",
    " \n",
    "        [[661, 556]],\n",
    " \n",
    "        [[662, 557]],\n",
    " \n",
    "        [[663, 557]],\n",
    " \n",
    "        [[665, 555]],\n",
    " \n",
    "        [[665, 553]],\n",
    " \n",
    "        [[664, 552]]], dtype=np.int32)\n",
    "#print(centers[0][0])\n",
    "distances = [i[0] for i in mindis(centers)] #sorted indices\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launches the cuboid transfer individual selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centers_cuboids = cont.contour_centers(cont.singular)\n",
    "grid = np.load('well_plate_96.npy')\n",
    "centers_cuboids = cont.contour_centers(cont.selected)\n",
    "#cub_offset = np.load('offset.npy')\n",
    "#x_off_base, y_off_base = base_offset\n",
    "dash.ResetRobot()\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "\n",
    "utils.default_pos(move)\n",
    "move.Sync()\n",
    "for idx, center in enumerate(centers_cuboids):\n",
    "    if idx == 96:\n",
    "        break # idx is index, enumerate takes an array and assigns indexes to each element of the array, each element in the array is a cuboid\n",
    "    X, Y, _ = tf_mtx @ (center[0], center[1], 1)\n",
    "    move.MovL(X, Y, -60, 0)\n",
    "    move.Sync()\n",
    "    # base_angle = utils.get_pose(dash, angle=True, verbose=False)[0]\n",
    "    # x_off, y_off = coordinate_rotation(y_off_base, x_off_base, base_angle)\n",
    "   \n",
    "    utils.correct_J4_angle(0, dash, move)\n",
    "    # utils.correct_J4_angle(-360, dash, move)\n",
    "    # utils.correct_J4_angle(0, dash, move)\n",
    "    move.RelMovL(0,0,-29)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(120, dash, move)\n",
    "    # move.RelMovL(0,0,29)\n",
    "    # move.Sync()\n",
    "    # x,y = grid[idx]\n",
    "    # move.MovL(x,y,-60, 120)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(120, dash, move)\n",
    "    # move.RelMovL(0,0,-23)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(-100, dash, move)\n",
    "    # move.RelMovL(0,0,23)\n",
    "    # move.Sync()\n",
    "    \n",
    "\n",
    "# utils.default_pos(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laser test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_cuboids = cont.contour_centers(cont.selected)\n",
    "grid = np.load('well_plate_96.npy')\n",
    "#centers_cuboids = cont.contour_centers(cont.selected)\n",
    "#cub_offset = np.load('offset.npy')\n",
    "#x_off_base, y_off_base = base_offset\n",
    "dash.ResetRobot()\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "\n",
    "utils.default_pos(move)\n",
    "move.Sync()\n",
    "for idx, center in enumerate(centers_cuboids):\n",
    "    if idx == 96:\n",
    "        break # idx is index, enumerate takes an array and assigns indexes to each element of the array, each element in the array is a cuboid\n",
    "    X, Y, _ = tf_mtx @ (center[0], center[1], 1)\n",
    "    move.MovL(X, Y, -89, 0)\n",
    "    move.Sync()\n",
    "    break\n",
    "    # base_angle = utils.get_pose(dash, angle=True, verbose=False)[0]\n",
    "    # x_off, y_off = coordinate_rotation(y_off_base, x_off_base, base_angle)\n",
    "   \n",
    "    # utils.correct_J4_angle(0, dash, move)\n",
    "    # # utils.correct_J4_angle(-360, dash, move)\n",
    "    # # utils.correct_J4_angle(0, dash, move)\n",
    "    # move.RelMovL(0,0,-29)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(120, dash, move)\n",
    "    # move.RelMovL(0,0,29)\n",
    "    # move.Sync()\n",
    "    # x,y = grid[idx]\n",
    "    # move.MovL(x,y,-60, 120)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(120, dash, move)\n",
    "    # move.RelMovL(0,0,-23)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(-100, dash, move)\n",
    "    # move.RelMovL(0,0,23)\n",
    "    # move.Sync()\n",
    "    \n",
    "\n",
    "#utils.default_pos(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('stock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8375749a1bafb386f34ea9f0dd7e55daf2784b2541aa2e32281e1f53a27462d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
