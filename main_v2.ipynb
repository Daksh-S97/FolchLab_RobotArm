{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.dobot_api import DobotApiDashboard, DobotApiMove # deps is a folder and we are importing 2 classes from a file within the folder, these classes \n",
    "# allow us to control the robot informationally and movement oriented \n",
    "from deps import utils # importing the module utils from a file within the folder deps, don't import all files from deps, just utils\n",
    "from deps import cv_core # this module houses all of the functions that are controlling and dealing with the camera and what it's doing\n",
    "import numpy as np # numpy is a module, we want lots of functions from numpy so we import full module\n",
    "import matplotlib.pyplot as plt # module that allows plotting, not used \n",
    "import cv2 # open cv, computer vision module with own GUI\n",
    "from pynput import keyboard # module allows you to get callbacks from keyboard press (control robot using arrow keys)\n",
    "import time # this module allows you to control timings \n",
    "import ipyparallel as ipp # module that allows code to be launched in parallel with other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.52s/engine]\n"
     ]
    }
   ],
   "source": [
    "rc = ipp.Cluster(n=1).start_and_connect_sync() # use this to move the robot with arrow keys and view camera feed at the same time \n",
    "e0 = rc[0]\n",
    "e0.block = False\n",
    "e0.activate('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash = DobotApiDashboard('192.168.1.6', 29999) # dash is the object that is connected to the robot and gets information from the dashboard\n",
    "move = DobotApiMove('192.168.1.6', 30003) # the object that allows you to control the movement of the robot\n",
    "# both classes we input ip address and port that we are communicating with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,{},EnableRobot();'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash.ClearError() # dash is an object and ClearError is a method in dash that we are calling, clears error and warning when robot freezes and turns red\n",
    "dash.EnableRobot() # enables the robot for use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,{},DisableRobot();'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash.DisableRobot() # disables the robot, white letters below is the robot's response as a String, 0 means that everything is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n",
      "Special key pressed: Key.esc\n"
     ]
    }
   ],
   "source": [
    "keys = utils.Keyboard(dash) # initializing the class Keyboard from the module (file that houses functions (methods) and classes utils and we are passing the parameter dash\n",
    "# we are giving the class Keyboard a connection to the robot which is called dash, dash is an object of the class dashboard\n",
    "keys.execute() # Keyboard has a method called execute, use this to record the position of the robot by pressing s. if finished press esc\n",
    "# we want to find corners of well plate so we record the position of the 4 corners and use this information later, just one use of execute of class keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([231.203172, -30.336811, -68.773483, -12.614088]),\n",
       " array([255.021442, -26.228175, -86.219749, -10.997171]),\n",
       " array([255.500729,  60.259733, -85.572372,   8.145549]),\n",
       " array([322.208784,  75.701264, -68.14045 ,   8.099175])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.coords # list of arrays with recorded coordinates at positions where s was pressed, coords is an attribute of keys (a variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [np.array([280,  40, -90,  0]), # list of positions from coords of 4 corners around petri dish, called anchors for the laser\n",
    "           np.array([230,  37, -90,  0]),\n",
    "           np.array([220, -14, -90,  0]),\n",
    "           np.array([270, -15, -90,  0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell calculates the grid for the 96 well plate\n",
    "well_plate = utils.assign_corners(keys.coords, reverse=True) # assign_corners is a method in the module utils \n",
    "left_side_points = np.linspace(well_plate['ul'], well_plate['ll'], 12)[:,:2]\n",
    "right_side_points = np.linspace(well_plate['ur'], well_plate['lr'], 12)[:,:2]\n",
    "grid = []\n",
    "for i in range(len(left_side_points)):\n",
    "    x1, y1 = left_side_points[i]\n",
    "    x2, y2 = right_side_points[i]\n",
    "    a = (x2-x1)/(y2-y1)\n",
    "    b = x1 - a*y1\n",
    "    ys = np.linspace(y1,y2,8) \n",
    "    xs = a*ys + b\n",
    "    grid += (list(zip(xs,ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('well_plate_96.npy',np.array(grid)) # saves well plate grid into a file named 'well_plate_96.npy', we want to save this so we can use later and not repeat process\n",
    "#np.load('well_plate_96.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.load('well_plate_96.npy')\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "for coord in grid:\n",
    "    x,y = coord\n",
    "    move.MovL(x,y,-60,0)\n",
    "    move.Sync()\n",
    "    move.MovL(x,y,-80,0)\n",
    "    move.Sync()\n",
    "    move.MovL(x,y,-60,0)\n",
    "    move.Sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n",
      "Position saved!\n"
     ]
    }
   ],
   "source": [
    "manmove = utils.ManualMove(move, dash) # used to control robot with keyboard, uses key presses to control robot \n",
    "manmove.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([312.783042, -16.318465, -89.99131 ,   0.      ]),\n",
       " array([323.838094,  48.267135, -89.977226,   0.      ]),\n",
       " array([274.033812,  48.260501, -89.964088,   0.      ]),\n",
       " array([264.664588, -15.289719, -89.950935,   0.      ])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manmove.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_offset = (manmove.coords[0] - np.array([250, 0, -90, 0]))[:2]\n",
    "np.save('base_offset.npy', base_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_rotation(x, y, angle):\n",
    "    xPrime = x*np.cos(angle) - y*np.sin(angle)\n",
    "    yPrime = x*np.sin(angle) + y*np.cos(angle)\n",
    "\n",
    "    return xPrime, yPrime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,{},MovL(250.000000,0.000000,-90.000000,0.000000);'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "move.MovL(250, 0, -90, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = 0.893852\n",
      "Y = 44.789589\n",
      "Z = 66.533707\n",
      "r = -0.893852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.893852, 44.789589, 66.533707, -0.893852])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_pose(dash, angle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%px0\n",
    "from deps import utils\n",
    "from deps.dobot_api import DobotApiDashboard, DobotApiMove\n",
    "dash = DobotApiDashboard('192.168.1.6', 29999)\n",
    "move = DobotApiMove('192.168.1.6', 30003)\n",
    "manmove = utils.ManualMove(move, dash)\n",
    "manmove.execute()\n",
    "coords = manmove.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([256.062558, -17.195119, -90.033806, -17.456869]),\n",
       " array([297.919911, -17.01596 , -90.018532, -17.456888]),\n",
       " array([304.430722,  37.666581, -90.009521, -17.456944]),\n",
       " array([263.810049,  37.101424, -89.996956, -17.457005])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = e0.pull('coords')\n",
    "coords = ar.get()\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2013.136] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@2026.272] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@2028.884] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ WARN:0@2031.051] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    }
   ],
   "source": [
    "# anchors = [np.array([307,  40, -90,  0]),\n",
    "#            np.array([260,  40, -90,  0]),\n",
    "#            np.array([250, -20, -90,  0]),\n",
    "#            np.array([300, -20, -90,  0])]\n",
    "\n",
    "anchors = [np.array([323.838094,  48.267135, -90,   0.      ]),\n",
    "            np.array([274.033812,  48.260501, -90,   0.      ]),\n",
    "            np.array([264.664588, -15.289719, -90,   0.      ]),\n",
    "            np.array([312.783042, -16.318465, -90,   0.      ])]\n",
    "\n",
    "# anchors = [np.array([290,  50, -90,  0]),\n",
    "#            np.array([240,  46, -90,  0]),\n",
    "#            np.array([230, -8, -90,  0]),\n",
    "#            np.array([280, -8, -90,  0])]\n",
    "\n",
    "# anchors = [np.array([306,  45, -90,  0]),\n",
    "#            np.array([255,  43, -90,  0]),\n",
    "#            np.array([247, -14, -90,  0]),\n",
    "#            np.array([295, -15, -90,  0])]\n",
    "\n",
    "# anchors = [np.array([280,  40, -90,  0]),\n",
    "#            np.array([230,  37, -90,  0]),\n",
    "#            np.array([220, -14, -90,  0]),\n",
    "#            np.array([270, -15, -90,  0])] # anchor positions, positions of the laser that the camera recognizes to create a transformation matrix\n",
    "                                          # allows you to transform pixel coordinates of an object to actual robot coordinates \n",
    "\n",
    "# computer vision stuff\n",
    "\n",
    "cameraMatrix = np.load('./cam_matrices/cam_mtx.npy')\n",
    "dist = np.load('./cam_matrices/dist.npy')\n",
    "newCameraMatrix = np.load('./cam_matrices/newcam_mtx.npy')\n",
    "template = cv2.imread('template.png', 0)\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200'])\n",
    "\n",
    "# cv2.namedWindow('frame',  cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow('frame', 1348, 1011)\n",
    "\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "\n",
    "\n",
    "recorded = []\n",
    "for anchor in anchors:\n",
    "    x,y,z,r = anchor\n",
    "    move.MovL(x,y,z,r)\n",
    "    move.Sync()\n",
    "\n",
    "    #while(True):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200'])\n",
    "\n",
    "    cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # creating a GUI window cv2 is a module called open cv which has all the methods related to computer vision\n",
    "    cv2.resizeWindow('frame', 1348, 1011)\n",
    "\n",
    "    ret, frame = cap.read() # how to access camera information, gives single frame that has been captured by the camera at the time of execution\n",
    "    # gives ret which is a boolean (true/false) true if frame captured, frame gives a numpy array that is basically the image (if color, 3 channel array rgb)\n",
    "    frame = cv2.undistort(frame, cameraMatrix, dist, None, newCameraMatrix) # need these 3 parameters to undistort the frame and give a new undistorted frame\n",
    "    plot_img = frame.copy() # create a copy of the variable frame (create a copy of the image)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # turn frame to grayscale \n",
    "    (minVal, maxVal, minLoc, maxLoc)=cv2.minMaxLoc(gray) # gives location of minimum and maximum pixel value, gives coordinates in pixels\n",
    "    a, b = maxLoc # unpack max location into 2 variables, the x and y (a and b) in pixels\n",
    "    \n",
    "    top_left = (a-w, b-h) # make white rectangle around desired maximum pixel values\n",
    "    bottom_right = (a+w, b+h)\n",
    "\n",
    "    mask = np.zeros_like(frame) # brightest part not always the center point, we want to isolate the bright spot, we apply a mask onto the image \n",
    "    # we take a grayscale image and we apply a mask around the bright spot, we create absolute black image \n",
    "    cv2.rectangle(mask,top_left, bottom_right, (255,255,255), -1) # we create filled white rectangle around the bright spot \n",
    "    cv2.rectangle(plot_img,top_left, bottom_right, (255,255,255), 2) # puts white hollow rectangle onto the visual image \n",
    "    result = cv2.bitwise_and(frame.astype('uint8'), mask.astype('uint8')) # we multiply the frame by the mask which leaves only the bright spot\n",
    "    gray_result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY) # result given in rgb and we want to go to grayscale, gives one grayscale array instead of 3 BGR\n",
    "    ret, thresh = cv2.threshold(gray_result,240,255,cv2.THRESH_BINARY) # apply thresholding to take brightest desired pixels and ignore all other values\n",
    "    M = cv2.moments(thresh) # this is what is finding the center by calculating the moments of the bright blob, calculates center of mass of a pixel blob\n",
    "    if M[\"m00\"] != 0:\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        cv2.circle(plot_img, (cX, cY), 5, (0, 255, 0), 2) # drawing a circle around the brightest blob for visual purposes \n",
    "        recorded.append((cX, cY)) # we record the x and y pixel values of the center of the blob so we can map the location onto the robots coordinates, at that point\n",
    "\n",
    "    cv2.imshow('frame',plot_img) # shows the image\n",
    "    cv2.waitKey(0) # index zero wait key, tells the computer to wait for any key press before continuing execution\n",
    "    cap.release() # releases the camera from control of the computer, disconnects the camera from the computer and empties memory\n",
    "   \n",
    "cv2.destroyAllWindows() # when 4 loop finishes it destroys (closes) the graphical window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cell that calculates the transformation matrix \n",
    "xys = [(arr[0], arr[1]) for arr in anchors]\n",
    "robot_coor = utils.assign_corners(xys, reverse=True) # assign corners to the robot coordinates at the 4 corner positions \n",
    "pix_coor = utils.assign_corners(recorded) # assign corners to the pixel coordinates at the 4 corner positions \n",
    "\n",
    "features_mm_to_pixels_dict = {} # setting up an empty dictionary to store the mapping of the corners from coordinate to pixel\n",
    "for key, value in robot_coor.items():\n",
    "    features_mm_to_pixels_dict[value] = pix_coor[key]\n",
    "\n",
    "\n",
    "tf_mtx = cv_core.compute_tf_mtx(features_mm_to_pixels_dict) # method of cv_core module that calculates transformation matrix\n",
    "# takes the dictionary and solves the system of linear equations that gives the transformation matrix and gives the actual relation between the pixels and millimeters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@10380.265] global /opt/conda/conda-bld/opencv-suite_1656606573658/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    }
   ],
   "source": [
    "cont = cv_core.Contours() # define class of methods for cuboid detection, initialize class\n",
    "\n",
    "def on_change(val): pass\n",
    "\n",
    "offset = 40 # create smaller inner circle in petri dish to locate cuboids \n",
    "cap = cv2.VideoCapture(0) # gets access to the camera \n",
    "cap = cv_core.set_res(cap, cv_core.camera_res_dict['1200']) # sets the resolution of the camera to 1200 x 1600\n",
    "\n",
    "cameraMatrix = np.load('./cam_matrices/cam_mtx.npy') # uploading the camera matrices for the calibration of the camera for undistortion\n",
    "dist = np.load('./cam_matrices/dist.npy') \n",
    "newCameraMatrix = np.load('./cam_matrices/newcam_mtx.npy') \n",
    "\n",
    "cv2.namedWindow('frame',  cv2.WINDOW_NORMAL) # create window\n",
    "cv2.resizeWindow('frame', 1348, 1011) # set resolution of window\n",
    "cv2.createTrackbar('Manual Lock', 'frame', 0, 1, on_change) # create trackbar between values of 0 and 1, basically a switch (manual lock of the circle recognition)\n",
    "# stops trying to recognize the petri dish (locked) increases performance of more fps because it recognizes a lot of circles which takes time so this locks it \n",
    "# circle recognition needs optimizing \n",
    "cv2.createTrackbar('Mask Offset', 'frame', offset, 150, on_change) # second trackbar allows control of offset which changes the size of the petri dish detection circle\n",
    "cv2.setMouseCallback('frame', cont.mousecallback) # set a callback function for double clicks of the mouse, this allows us to select cuboids by double clicking\n",
    "# initializes a double click response to select a certain contour on the screen \n",
    "\n",
    "idx = 0 \n",
    "prev_point = (0,0,0)\n",
    "while(True): # we want a video steam so we want a while loop to continuously take new images until loop is broken\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.undistort(frame, cameraMatrix, dist, None, newCameraMatrix)\n",
    "    val = cv2.getTrackbarPos('Manual Lock', 'frame') # checks position of trackbar for manual lock, if trackbar 1, then val = 1, this turns off circle detection\n",
    "    offset = cv2.getTrackbarPos('Mask Offset', 'frame') # the trackbar with the offset, changes the offset value for the circle\n",
    "    if val == 1:\n",
    "        cont.locked = True\n",
    "    else:\n",
    "        cont.locked = False\n",
    "\n",
    "    # if cont.best_circ is None:\n",
    "    #     while cont.best_circ is None:\n",
    "    #         pt = cont.find_contours(frame, 10, offset)\n",
    "    # else:\n",
    "    pt = cont.find_contours(frame, 10, offset) # takes the frame and looks for the circle of the petri dish, also looks for the cuboids \n",
    "        \n",
    "    a,b,r = pt # gives the x y coordinates and the radius of the circle\n",
    "    plot_img = frame.copy() # create a copy of the frame so things can be drawn on it without altering original image\n",
    "    cv2.circle(plot_img, (a, b), r, (3, 162, 255), 2) # circles being drawn on image to locate petri dish\n",
    "    cv2.circle(plot_img, (a, b), 1, (0, 0, 255), 3)\n",
    "    cv2.circle(plot_img, (a, b), r - offset, (0, 255, 0), 2)\n",
    "    cv2.circle(plot_img, cont.big_circ[:2], cont.big_circ[2], (0, 0, 255), 2)\n",
    "    cv2.putText(plot_img, f\"{cont.big_circ[2]*2}px = 60mm\", (cont.big_circ[0]-25, cont.big_circ[1] - cont.big_circ[2] - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2) # how to decide size of cuboids, use diameter of petri dish being 60 mm to approximate size of cuboids\n",
    "        # just a text sign around the biggest circle of the image (petri dish) saying that this circle is 60 mm\n",
    "    \n",
    "    cv2.drawContours(plot_img, cont.singular, -1,(0,255, 0),2) # two lists of contours (cuboid contours) that we draw on the plotting image\n",
    "    cv2.drawContours(plot_img, cont.clusters, -1,(0,0,255),2) # we do this to distinguish between clusters and cuboids \n",
    "\n",
    "    if cont.selected: # if we selected cuboids with the double click, we draw them in blue\n",
    "        cv2.drawContours(plot_img, cont.selected, -1,(255,0,0),2) # BGR, this is blue contour \n",
    "\n",
    "    for c in cont.singular: # we find the centers of the contours similar to how we found centers with the calibration step\n",
    "        # compute the center of the contour\n",
    "        M = cv2.moments(c)\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"]) # the function moments gives a mysterious dictionary with elements of matrix, it seems that you gotta divide one element by another to get x&y\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        \n",
    "        cv2.circle(plot_img, (cX, cY), 2, (0, 0, 255), -1) # just draw a cirlce around the contour \n",
    "        # cv2.putText(plot_img, f\"{cX},{cY}\", (cX - 20, cY - 20),\n",
    "        # cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(plot_img, f\"{cv2.contourArea(c)}\", (cX - 20, cY - 20), # putting the area of the cuboids onto the screen as text next to the cuboid\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 1)\n",
    "    #out.write(with_contours)\n",
    "\n",
    "    for c in cont.clusters: # same thing for clustes as above but no printed text for cuboid area or coordinate\n",
    "        # compute the center of the contour\n",
    "        M = cv2.moments(c)\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        # draw the contour and center of the shape on the image\n",
    "        #cv2.drawContours(plot_img, [c], -1, (0, 255, 0), 2)\n",
    "        cv2.circle(plot_img, (cX, cY), 2, (0, 0, 255), -1)\n",
    "        # cv2.putText(plot_img, f\"{cX},{cY}\", (cX - 20, cY - 20),\n",
    "        # cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 255, 0), 1)\n",
    "\n",
    "        # cv2.putText(plot_img, f\"{cv2.contourArea(c)}\", (cX - 20, cY - 20),\n",
    "        # cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "    if prev_point is pt:\n",
    "        idx += 1\n",
    "    else:\n",
    "        idx = 0\n",
    "    prev_point = pt\n",
    "\n",
    "\n",
    "    if idx >= 30: # if circle stays same for awhile then it will show as locked (30 frames the same)\n",
    "        message = 'LOCKED'\n",
    "        color = (0,255,0)\n",
    "    else:\n",
    "        message = 'SEARCHING' # more text messages \n",
    "        color = (0,0,255)\n",
    "\n",
    "    cv2.putText(plot_img, \"TARGET:\", (25,25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2) # text messages, target displays constantly\n",
    "    cv2.putText(plot_img, f\"{message}\", (125,25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2) # displays message locked or searching\n",
    "    cv2.putText(plot_img, f\"Found: {len(cont.singular)}\", (25,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2) # how many cuboids it sees\n",
    "\n",
    "    \n",
    "    cv2.imshow('frame',plot_img) # this just displays the image \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # if you press q, the while loop will break and the video and locating will stop, then the camera is released and windows destroyed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "M = cv2.moments(cont.selected[0])\n",
    "cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "X, Y, _ = tf_mtx @ (cX, cY, 1)\n",
    "move.MovL(X, Y, -90, 0)\n",
    "move.Sync()\n",
    "utils.correct_J4_angle(0, dash, move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "manmove = utils.ManualMove(move, dash)\n",
    "manmove.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,{},DisableRobot();'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dash.DisableRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centers_cuboids = cont.contour_centers(cont.singular)\n",
    "centers_cuboids = cont.contour_centers(cont.selected)\n",
    "cub_offset = np.load('offset.npy')\n",
    "x_off_base, y_off_base = base_offset\n",
    "dash.ClearError()\n",
    "dash.EnableRobot()\n",
    "\n",
    "utils.default_pos(move)\n",
    "move.Sync()\n",
    "for idx, center in enumerate(centers_cuboids):\n",
    "    X, Y, _ = tf_mtx @ (center[0], center[1], 1)\n",
    "    move.MovL(X, Y, -60, 0)\n",
    "    move.Sync()\n",
    "    base_angle = utils.get_pose(dash, angle=True, verbose=False)[0]\n",
    "    x_off, y_off = coordinate_rotation(y_off_base, x_off_base, base_angle)\n",
    "    move.MovL(X+x_off, Y+y_off, -60, 0)\n",
    "    move.Sync()\n",
    "    #utils.correct_J4_angle(0, dash, move)\n",
    "    # utils.correct_J4_angle(-360, dash, move)\n",
    "    # utils.correct_J4_angle(0, dash, move)\n",
    "    move.RelMovL(0,0,-30)\n",
    "    move.Sync()\n",
    "    #utils.correct_J4_angle(120, dash, move)\n",
    "    # move.RelMovL(0,0,30)\n",
    "    # move.Sync()\n",
    "    # x,y = grid[idx]\n",
    "    # move.MovL(x,y,-60, 120)\n",
    "    # move.Sync()\n",
    "    # #utils.correct_J4_angle(120, dash, move)\n",
    "    # move.RelMovL(0,0,-30)\n",
    "    # move.Sync()\n",
    "    # #utils.correct_J4_angle(-50, dash, move)\n",
    "    # move.RelMovL(0,0,30)\n",
    "    # move.Sync()\n",
    "    # x,y,z,r = utils.get_pose(dash)\n",
    "    # # move.MovL(X,Y,-60,r+angles[0])\n",
    "    # # move.Sync()\n",
    "    # move.MovL(X+x_off, Y+y_off,-89.5,r)\n",
    "    # move.Sync()\n",
    "    # move.MovL(X+x_off, Y+y_off, -89.5,r+160)\n",
    "    # move.Sync()\n",
    "    # move.MovL(X+x_off, Y+y_off, -60, r+160)\n",
    "    # move.Sync()\n",
    "    # x,y = grid[idx+8]\n",
    "    # move.MovL(x,y,-60, r+160)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(r+160, dash, move)\n",
    "    # move.Sync()\n",
    "    # x,y,z,r_new = utils.get_pose(dash)\n",
    "    # move.MovL(x, y, -85, r_new)\n",
    "    # move.Sync()\n",
    "    # utils.correct_J4_angle(0, dash, move)\n",
    "    # move.Sync()\n",
    "    # x,y,z,r_new = utils.get_pose(dash)\n",
    "    # # move.Sync()\n",
    "    # move.MovL(x, y, -60, r_new)\n",
    "    # move.Sync()\n",
    "\n",
    "#utils.default_pos(move)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('stock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8375749a1bafb386f34ea9f0dd7e55daf2784b2541aa2e32281e1f53a27462d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
